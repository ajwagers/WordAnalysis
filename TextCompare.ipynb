{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import math\n",
    "\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from scipy.optimize import curve_fit\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, clear_output\n",
    "from textstat import flesch_reading_ease, gunning_fog, smog_index\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "#paulbunyan = 'https://www.gutenberg.org/cache/epub/70060/pg70060.txt'\n",
    "#Americanfairytales = 'https://www.gutenberg.org/ebooks/4357.txt.utf-8'\n",
    "#merryadventuresofRobinHood = 'https://www.gutenberg.org/ebooks/10148.txt.utf-8'\n",
    "#legendsofkingarthurandhisknight = 'https://www.gutenberg.org/ebooks/12753.txt.utf-8'\n",
    "#mythsandlegendsofancientgreece = 'https://www.gutenberg.org/ebooks/22381.txt.utf-8'\n",
    "\n",
    "\n",
    "#shortlist = [paulbunyan,Americanfairytales,merryadventuresofRobinHood,legendsofkingarthurandhisknight,mythsandlegendsofancientgreece]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_book(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def strip_gutenberg_text(text):\n",
    "    # Remove text before the start of the book\n",
    "    start_markers = [\n",
    "        \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"*END*THE SMALL PRINT!\"\n",
    "    ]\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            text = text.split(marker, 1)[-1]\n",
    "            break\n",
    "    \n",
    "    # Remove text after the end of the book\n",
    "    end_markers = [\n",
    "        \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"End of Project Gutenberg's\"\n",
    "    ]\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            text = text.split(marker, 1)[0]\n",
    "            break\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_title_author(text):\n",
    "    title_pattern = r\"Title: (.+)\"\n",
    "    author_pattern = r\"Author: (.+)\"\n",
    "    \n",
    "    title_match = re.search(title_pattern, text)\n",
    "    author_match = re.search(author_pattern, text)\n",
    "    \n",
    "    title = title_match.group(1) if title_match else \"Unknown Title\"\n",
    "    author = author_match.group(1) if author_match else \"Unknown Author\"\n",
    "    \n",
    "    return title.strip(), author.strip()\n",
    "\n",
    "def process_book(url):\n",
    "    raw_text = fetch_book(url)\n",
    "    title, author = extract_title_author(raw_text)\n",
    "    cleaned_text = strip_gutenberg_text(raw_text)\n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'text': cleaned_text\n",
    "    }\n",
    "\n",
    "# List of book URLs\n",
    "book_urls = [\n",
    "    'https://www.gutenberg.org/cache/epub/70060/pg70060.txt',\n",
    "    'https://www.gutenberg.org/ebooks/4357.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/10148.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/12753.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/22381.txt.utf-8'\n",
    "]\n",
    "\n",
    "# Process all books\n",
    "processed_books = []\n",
    "for url in book_urls:\n",
    "    try:\n",
    "        book_data = process_book(url)\n",
    "        processed_books.append(book_data)\n",
    "        print(f\"Processed: {book_data['title']} by {book_data['author']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "# Now 'processed_books' contains a list of dictionaries, each with 'url', 'title', 'author', and 'text' keys\n",
    "# Each book's text is kept separate in its own dictionary\n",
    "\n",
    "# Example of how to access the text of a specific book:\n",
    "#if processed_books:\n",
    "#    first_book = processed_books[0]\n",
    "#    print(f\"\\nFirst few characters of '{first_book['title']}' by {first_book['author']}:\")\n",
    "#    print(first_book['text'][:200])  # Print first 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_wordcloud(text, title, output_dir):\n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add punctuation to stopwords\n",
    "    stop_words.update(string.punctuation)\n",
    "\n",
    "    # Tokenize the text and remove stopwords and punctuation\n",
    "    words = [word.lower() for word in nltk.word_tokenize(text) \n",
    "             if word.lower() not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                          stopwords=stop_words, min_font_size=10).generate(cleaned_text)\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the generated image\n",
    "    filename = f\"{title.replace(' ', '_')}_wordcloud.png\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    wordcloud.to_file(filepath)\n",
    "    print(f\"Word cloud saved as: {filepath}\")\n",
    "\n",
    "    # Display the generated image in the notebook\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud: {title}\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Generate and save word clouds in a separate loop\n",
    "print(\"\\nGenerating word clouds:\")\n",
    "output_directory = \"wordclouds\"\n",
    "for book in processed_books:\n",
    "    try:\n",
    "        print(f\"Generating word cloud for: {book['title']}\")\n",
    "        generate_and_save_wordcloud(book['text'], book['title'], output_directory)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating word cloud for {book['title']}: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll word clouds generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_book(book):\n",
    "    text = book['text']\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add punctuation to stopwords\n",
    "    stop_words.update(string.punctuation)\n",
    "\n",
    "    # Tokenize the text and remove stopwords and punctuation\n",
    "    words = [word.lower() for word in nltk.word_tokenize(text) \n",
    "             if word.lower() not in stop_words \n",
    "             and word not in string.punctuation\n",
    "             and word.isalpha()]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    text = cleaned_text\n",
    "\n",
    "    # Word frequency\n",
    "    freq_dist = FreqDist(words)\n",
    "    top_words = freq_dist.most_common(20)\n",
    "    \n",
    "    # Readability\n",
    "    flesch_score = flesch_reading_ease(text)\n",
    "    gunning_fog_score = gunning_fog(text)\n",
    "    smog_score = smog_index(text)\n",
    "    \n",
    "    # Average sentence length\n",
    "    avg_sentence_length = np.mean([len(word_tokenize(sent)) for sent in sentences])\n",
    "    \n",
    "    # Sentence length variation\n",
    "    sentence_length_variation = np.std([len(word_tokenize(sent)) for sent in sentences])\n",
    "    \n",
    "    # Type-Token Ratio\n",
    "    type_token_ratio = len(set(words)) / len(words)\n",
    "    \n",
    "    # POS distribution\n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_counts = FreqDist(tag for (word, tag) in pos_tags)\n",
    "    \n",
    "    # Sentiment scores\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [sia.polarity_scores(sent) for sent in sentences]\n",
    "    avg_sentiment = np.mean([score['compound'] for score in sentiment_scores])\n",
    "    \n",
    "    # Dialogue ratio (rough estimate based on quotation marks)\n",
    "    dialogue_words = sum(len(word_tokenize(sent)) for sent in sentences if sent.count('\"') >= 2)\n",
    "    dialogue_ratio = dialogue_words / len(words)\n",
    "    \n",
    "    # Named Entities\n",
    "    chunked_sentences = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    named_entities = [\" \".join(word for word, pos in subtree.leaves())\n",
    "                      for subtree in chunked_sentences\n",
    "                      if isinstance(subtree, nltk.Tree)]\n",
    "    top_named_entities = FreqDist(named_entities).most_common(10)\n",
    "    \n",
    "    return {\n",
    "        'title': book['title'],\n",
    "        'flesch_score': flesch_score,\n",
    "        'gunning_fog_score': gunning_fog_score,\n",
    "        'smog_score': smog_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'sentence_length_variation': sentence_length_variation,\n",
    "        'type_token_ratio': type_token_ratio,\n",
    "        'pos_distribution': dict(pos_counts),\n",
    "        'sentiment_compound': avg_sentiment,\n",
    "        'dialogue_ratio': dialogue_ratio,\n",
    "        'word_frequency': dict(top_words),\n",
    "        'named_entities': dict(top_named_entities)\n",
    "    }\n",
    "\n",
    "# Analyze all books\n",
    "book_analyses = [analyze_book(book) for book in processed_books]\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Title': analysis['title'],\n",
    "        'Flesch Reading Ease': analysis['flesch_score'],\n",
    "        'Gunning Fog Index': analysis['gunning_fog_score'],\n",
    "        'SMOG Index': analysis['smog_score'],\n",
    "        'Avg Sentence Length': analysis['avg_sentence_length'],\n",
    "        'Sentence Length Variation': analysis['sentence_length_variation'],\n",
    "        'Type-Token Ratio': analysis['type_token_ratio'],\n",
    "        'Avg Sentiment': analysis['sentiment_compound'],\n",
    "        'Dialogue Ratio': analysis['dialogue_ratio']\n",
    "    }\n",
    "    for analysis in book_analyses\n",
    "])\n",
    "\n",
    "# Display the summary comparison table\n",
    "print(\"Summary Comparison Table:\")\n",
    "display(df)\n",
    "\n",
    "# Display detailed metrics for each book\n",
    "print(\"\\nDetailed Metrics for Each Book:\")\n",
    "for analysis in book_analyses:\n",
    "    print(f\"\\nTitle: {analysis['title']}\")\n",
    "    print(f\"Flesch Reading Ease: {analysis['flesch_score']:.2f}\")\n",
    "    print(f\"Gunning Fog Index: {analysis['gunning_fog_score']:.2f}\")\n",
    "    print(f\"SMOG Index: {analysis['smog_score']:.2f}\")\n",
    "    print(f\"Average Sentence Length: {analysis['avg_sentence_length']:.2f}\")\n",
    "    print(f\"Sentence Length Variation: {analysis['sentence_length_variation']:.2f}\")\n",
    "    print(f\"Type-Token Ratio: {analysis['type_token_ratio']:.4f}\")\n",
    "    print(f\"Dialogue Ratio: {analysis['dialogue_ratio']:.4f}\")\n",
    "    print(f\"Sentiment Compound Score: {analysis['sentiment_compound']:.4f}\")\n",
    "    print(\"\\nTop 20 Most Frequent Words:\")\n",
    "    for word, count in analysis['word_frequency'].items():\n",
    "        print(f\"  {word}: {count}\")\n",
    "    print(\"\\nPart-of-Speech Distribution:\")\n",
    "    for pos, count in analysis['pos_distribution'].items():\n",
    "        print(f\"  {pos}: {count}\")\n",
    "    print(\"\\nTop 10 Named Entities:\")\n",
    "    for entity, count in analysis['named_entities'].items():\n",
    "        print(f\"  {entity}: {count}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Visualizations\n",
    "\n",
    "# 1. Word Frequency\n",
    "num_books = len(book_analyses)\n",
    "num_cols = 5\n",
    "num_rows = math.ceil(num_books / num_cols)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, analysis in enumerate(book_analyses, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    words = list(analysis['word_frequency'].keys())[:10]\n",
    "    counts = list(analysis['word_frequency'].values())[:10]\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(f\"Top 10 Words in {analysis['title']}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Readability Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='Flesch Reading Ease', y='Gunning Fog Index', hue='Title')\n",
    "plt.title('Readability Scores Comparison')\n",
    "plt.show()\n",
    "\n",
    "# 3. Sentence Length and Variation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='Avg Sentence Length', y='Sentence Length Variation', hue='Title')\n",
    "plt.title('Sentence Length and Variation')\n",
    "plt.show()\n",
    "\n",
    "# 4. POS Distribution\n",
    "pos_df = pd.DataFrame([analysis['pos_distribution'] for analysis in book_analyses])\n",
    "pos_df.index = [analysis['title'] for analysis in book_analyses]\n",
    "pos_df = pos_df.div(pos_df.sum(axis=1), axis=0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pos_df, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "plt.title('POS Distribution Comparison')\n",
    "plt.show()\n",
    "\n",
    "# 5. Sentiment and Dialogue Ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='Avg Sentiment', y='Dialogue Ratio', hue='Title')\n",
    "plt.title('Sentiment vs Dialogue Ratio')\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis complete. Check the visualizations above for comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#talltalesfromTexas = 'https://www.gutenberg.org/ebooks/71089.txt.utf-8'\n",
    "#mythsandfolktalesoftheRussians = 'https://www.gutenberg.org/ebooks/50011.txt.utf-8'\n",
    "#talesofCapeCod = 'https://www.gutenberg.org/ebooks/69718.txt.utf-8'\n",
    "#surprisingadventuresofBaronMunchausen = 'https://www.gutenberg.org/ebooks/3154.txt.utf-8'\n",
    "#ConnecticutyankeeinKingArthurscourt = 'https://www.gutenberg.org/ebooks/86.txt.utf-8'\n",
    "#adventuresofHuckleberryFinn = 'https://www.gutenberg.org/ebooks/76.txt.utf-8'\n",
    "#casebookofSherlockHolmes = 'https://www.gutenberg.org/ebooks/69700.txt.utf-8'\n",
    "#lostworld = 'https://www.gutenberg.org/ebooks/139.txt.utf-8'\n",
    "#wonderfulwiardofOz = 'https://www.gutenberg.org/ebooks/55.txt.utf-8'\n",
    "#phantastesafaerieromanceformenandwomen = 'https://www.gutenberg.org/ebooks/325.txt.utf-8'\n",
    "#aesopsfables = 'https://www.gutenberg.org/ebooks/11339.txt.utf-8'\n",
    "#manwhowasthursdayanightmare = 'https://www.gutenberg.org/ebooks/1695.txt.utf-8'\n",
    "#romeoandjuliet = 'https://www.gutenberg.org/ebooks/1513.txt.utf-8'\n",
    "#twentythousandleaguesunderthesea = 'https://www.gutenberg.org/ebooks/164.txt.utf-8'\n",
    "#aroundtheworldin80days = 'https://www.gutenberg.org/ebooks/103.txt.utf-8'\n",
    "#journeytothecenteroftheearth = 'https://www.gutenberg.org/ebooks/18857.txt.utf-8'\n",
    "#warandpeace = 'https://www.gutenberg.org/ebooks/2600.txt.utf-8'\n",
    "#annakarenina = 'https://www.gutenberg.org/ebooks/1399.txt.utf-8'\n",
    "#crimeandpunishment = 'https://www.gutenberg.org/ebooks/2554.txt.utf-8'\n",
    "#brotherskaramazov = 'https://www.gutenberg.org/ebooks/28054.txt.utf-8'\n",
    "#lesmiserables = 'https://www.gutenberg.org/ebooks/135.txt.utf-8'\n",
    "#frankenstein = 'https://www.gutenberg.org/ebooks/84.txt.utf-8'\n",
    "#dracula = 'https://www.gutenberg.org/ebooks/345.txt.utf-8'\n",
    "#pilgrimsprogress = 'https://www.gutenberg.org/ebooks/131.txt.utf-8'\n",
    "\n",
    "#longlist = [paulbunyan,talltalesfromTexas,mythsandfolktalesoftheRussians,talesofCapeCod,surprisingadventuresofBaronMunchausen,Americanfairytales,wonderfulwiardofOz,\n",
    "#            merryadventuresofRobinHood,ConnecticutyankeeinKingArthurscourt,adventuresofHuckleberryFinn,casebookofSherlockHolmes,lostworld,legendsofkingarthurandhisknight,\n",
    "#            mythsandfolktalesoftheRussians,phantastesafaerieromanceformenandwomen,aesopsfables,manwhowasthursdayanightmare,romeoandjuliet,twentythousandleaguesunderthesea,\n",
    "#            aroundtheworldin80days,journeytothecenteroftheearth,warandpeace,annakarenina,crimeandpunishment,brotherskaramazov,lesmiserables,frankenstein,dracula,pilgrimsprogress]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code section after the existing analysis\n",
    "\n",
    "# New book URLs\n",
    "new_book_urls = [\n",
    "    'https://www.gutenberg.org/cache/epub/70060/pg70060.txt',\n",
    "    'https://www.gutenberg.org/ebooks/4357.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/10148.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/12753.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/22381.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/71089.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/50011.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69718.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/3154.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/86.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/76.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69700.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/139.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/55.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/325.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/11339.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/1695.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/1513.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/164.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/103.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/18857.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/2600.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/1399.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/2554.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/28054.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/135.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/84.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/345.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/131.txt.utf-8'\n",
    "]\n",
    "\n",
    "# Process new books\n",
    "new_processed_books = []\n",
    "for url in new_book_urls:\n",
    "    try:\n",
    "        book_data = process_book(url)\n",
    "        new_processed_books.append(book_data)\n",
    "        print(f\"Processed: {book_data['title']} by {book_data['author']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "# Generate word clouds for new books\n",
    "print(\"\\nGenerating word clouds for new books:\")\n",
    "for book in new_processed_books:\n",
    "    try:\n",
    "        print(f\"Generating word cloud for: {book['title']}\")\n",
    "        generate_and_save_wordcloud(book['text'], book['title'], output_directory)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating word cloud for {book['title']}: {str(e)}\")\n",
    "\n",
    "# Analyze new books\n",
    "new_book_analyses = [analyze_book(book) for book in new_processed_books]\n",
    "\n",
    "# Create a DataFrame for new books\n",
    "new_df = pd.DataFrame([\n",
    "    {\n",
    "        'Title': analysis['title'],\n",
    "        'Flesch Reading Ease': analysis['flesch_score'],\n",
    "        'Gunning Fog Index': analysis['gunning_fog_score'],\n",
    "        'SMOG Index': analysis['smog_score'],\n",
    "        'Avg Sentence Length': analysis['avg_sentence_length'],\n",
    "        'Sentence Length Variation': analysis['sentence_length_variation'],\n",
    "        'Type-Token Ratio': analysis['type_token_ratio'],\n",
    "        'Avg Sentiment': analysis['sentiment_compound'],\n",
    "        'Dialogue Ratio': analysis['dialogue_ratio']\n",
    "    }\n",
    "    for analysis in new_book_analyses\n",
    "])\n",
    "\n",
    "# Display the summary comparison table for new books\n",
    "print(\"Summary Comparison Table for New Books:\")\n",
    "print(new_df)\n",
    "\n",
    "# Display detailed metrics for each new book\n",
    "print(\"\\nDetailed Metrics for Each New Book:\")\n",
    "for analysis in new_book_analyses:\n",
    "    print(f\"\\nTitle: {analysis['title']}\")\n",
    "    print(f\"Flesch Reading Ease: {analysis['flesch_score']:.2f}\")\n",
    "    print(f\"Gunning Fog Index: {analysis['gunning_fog_score']:.2f}\")\n",
    "    print(f\"SMOG Index: {analysis['smog_score']:.2f}\")\n",
    "    print(f\"Average Sentence Length: {analysis['avg_sentence_length']:.2f}\")\n",
    "    print(f\"Sentence Length Variation: {analysis['sentence_length_variation']:.2f}\")\n",
    "    print(f\"Type-Token Ratio: {analysis['type_token_ratio']:.4f}\")\n",
    "    print(f\"Dialogue Ratio: {analysis['dialogue_ratio']:.4f}\")\n",
    "    print(f\"Sentiment Compound Score: {analysis['sentiment_compound']:.4f}\")\n",
    "    print(\"\\nTop 20 Most Frequent Words:\")\n",
    "    for word, count in analysis['word_frequency'].items():\n",
    "        print(f\"  {word}: {count}\")\n",
    "    print(\"\\nPart-of-Speech Distribution:\")\n",
    "    for pos, count in analysis['pos_distribution'].items():\n",
    "        print(f\"  {pos}: {count}\")\n",
    "    print(\"\\nTop 10 Named Entities:\")\n",
    "    for entity, count in analysis['named_entities'].items():\n",
    "        print(f\"  {entity}: {count}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Combine old and new dataframes\n",
    "all_df = pd.concat([df, new_df])\n",
    "\n",
    "# Visualizations for all books\n",
    "\n",
    "# 1. Word Frequency\n",
    "all_analyses = book_analyses + new_book_analyses\n",
    "num_books = len(all_analyses)\n",
    "num_cols = 5\n",
    "num_rows = math.ceil(num_books / num_cols)\n",
    "\n",
    "plt.figure(figsize=(20, 4 * num_rows))\n",
    "for i, analysis in enumerate(all_analyses, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    words = list(analysis['word_frequency'].keys())[:10]\n",
    "    counts = list(analysis['word_frequency'].values())[:10]\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(f\"Top 10 Words in {analysis['title'][:20]}...\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Readability Scores\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=all_df, x='Flesch Reading Ease', y='Gunning Fog Index', hue='Title')\n",
    "plt.title('Readability Scores Comparison (All Books)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Sentence Length and Variation\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=all_df, x='Avg Sentence Length', y='Sentence Length Variation', hue='Title')\n",
    "plt.title('Sentence Length and Variation (All Books)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. POS Distribution\n",
    "all_pos_df = pd.DataFrame([analysis['pos_distribution'] for analysis in book_analyses + new_book_analyses])\n",
    "all_pos_df.index = [analysis['title'] for analysis in book_analyses + new_book_analyses]\n",
    "all_pos_df = all_pos_df.div(all_pos_df.sum(axis=1), axis=0)\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(all_pos_df, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "plt.title('POS Distribution Comparison (All Books)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Sentiment and Dialogue Ratio\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=all_df, x='Avg Sentiment', y='Dialogue Ratio', hue='Title')\n",
    "plt.title('Sentiment vs Dialogue Ratio (All Books)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis complete for all books. Check the visualizations for comparisons.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
