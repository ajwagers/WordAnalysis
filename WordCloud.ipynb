{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "import spacy\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from scipy.optimize import curve_fit\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, clear_output\n",
    "from textstat import flesch_reading_ease, gunning_fog, smog_index\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Get the text of 'Paul Bunyan' by James Stevens from Project Gutenberg\n",
    "r = requests.get('https://www.gutenberg.org/cache/epub/70060/pg70060.txt')\n",
    "r.encoding = 'utf-8'\n",
    "booktext = r.text\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Project Gutenberg header\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK PAUL BUNYAN ***\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK PAUL BUNYAN ***\"\n",
    "    start_index = text.find(start_marker) + len(start_marker)\n",
    "    if start_index == -1:\n",
    "        start_index = 0\n",
    "    else:\n",
    "        start_index = text.index(\"\\n\", start_index) + 1\n",
    "    end_index = text.find(end_marker)\n",
    "    if end_index == -1:\n",
    "        end_index = len(text)\n",
    "    text = text[start_index:end_index]\n",
    "\n",
    "    # Remove the mentions of illustrations\n",
    "    text = re.sub(r'\\[Illustration\\]', '', text)\n",
    "    # Remove punctuation but keep capitaliation for name recognition\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(booktext)\n",
    "\n",
    "# Split into words\n",
    "words = cleaned_text.split()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "additional_stopwords = ['said','now','one']\n",
    "stopwords.update(additional_stopwords)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=set(STOPWORDS),\n",
    "        max_words=100,\n",
    "        max_font_size=88, \n",
    "        scale=3,\n",
    "        random_state=random.randint(0,1000)\n",
    "    ).generate(str(data))\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(cleaned_text, 'Paul Bunyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "words = [word.lower() for word in tokens if word.lower() not in stopwords]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['frequency'])\n",
    "df = df.sort_values('frequency', ascending=False)\n",
    "df['word'] = df.index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Calculate additional statistics\n",
    "total_words = len(words)\n",
    "df['length'] = df['word'].str.len()\n",
    "df['frequency_percentage'] = df['frequency'] / total_words * 100\n",
    "df['normalized_frequency_1000'] = df['frequency'] / total_words * 1000  # Normalized per 1000 words\n",
    "\n",
    "# Print the table\n",
    "print(df.head(20).to_string(index=False))\n",
    "\n",
    "# Calculate and print some overall statistics\n",
    "unique_words = len(df)\n",
    "average_word_length = df['length'].mean()\n",
    "\n",
    "# Calculate the Type-Token Ratio\n",
    "ttr = unique_words / total_words\n",
    "\n",
    "# Advanced Lexical Diversity Measures\n",
    "def mtld(text, ttr_threshold=0.72):\n",
    "    def mtld_calc(text, ttr_threshold):\n",
    "        factors = 0\n",
    "        types = set()\n",
    "        tokens = 0\n",
    "        for word in text:\n",
    "            types.add(word)\n",
    "            tokens += 1\n",
    "            ttr = len(types) / tokens\n",
    "            if ttr <= ttr_threshold:\n",
    "                factors += 1\n",
    "                types = set()\n",
    "                tokens = 0\n",
    "        if tokens > 0:\n",
    "            factors += (1 - ttr) / (1 - ttr_threshold)\n",
    "        return len(text) / factors if factors > 0 else 0\n",
    "    \n",
    "    forward = mtld_calc(text, ttr_threshold)\n",
    "    backward = mtld_calc(text[::-1], ttr_threshold)\n",
    "    return (forward + backward) / 2\n",
    "\n",
    "def mattr(text, window_size=1000):\n",
    "    ttrs = []\n",
    "    for i in range(len(text) - window_size + 1):\n",
    "        window = text[i:i+window_size]\n",
    "        ttrs.append(len(set(window)) / len(window))\n",
    "    return np.mean(ttrs)\n",
    "\n",
    "def mass_function_residual(text, num_points=10):\n",
    "    def expected_types(n, a, b):\n",
    "        return a * n ** b\n",
    "\n",
    "    def calculate_observed_types(text, num_points):\n",
    "        observed_types = []\n",
    "        step = len(text) // num_points\n",
    "        for i in range(step, len(text) + 1, step):\n",
    "            observed_types.append(len(set(text[:i])))\n",
    "        return observed_types, step\n",
    "\n",
    "    observed_types, step = calculate_observed_types(text, num_points)\n",
    "    x = np.arange(step, len(text) + 1, step)\n",
    "    \n",
    "    # Fit the expected function to the observed data\n",
    "    popt, _ = curve_fit(expected_types, x, observed_types)\n",
    "    \n",
    "    # Calculate the residuals\n",
    "    expected = expected_types(x, *popt)\n",
    "    residuals = np.array(observed_types) - expected\n",
    "    \n",
    "    # Normalize the residuals\n",
    "    normalized_residuals = residuals / expected\n",
    "    \n",
    "    # Return the mean of the absolute normalized residuals\n",
    "    return np.mean(np.abs(normalized_residuals))\n",
    "\n",
    "# Calculate advanced measures\n",
    "mtld_score = mtld(words)\n",
    "mattr_score = mattr(words)\n",
    "mfr_score = mass_function_residual(words)\n",
    "\n",
    "# Print Statistics\n",
    "print(f\"\\nTotal words (tokens): {total_words}\")\n",
    "print(f\"Unique words (types): {unique_words}\")\n",
    "print(f\"Type-Token Ratio: {ttr:.4f}\")\n",
    "print(f\"MATTR: {mattr_score:.4f}\")\n",
    "print(f\"MTLD: {mtld_score:.4f}\")\n",
    "print(f\"Mass-Function Residual: {mfr_score:.4f}\")\n",
    "print(f\"Average word length: {df['length'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_top_words(df, n=20, normalized=False):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    if normalized:\n",
    "        sns.barplot(x='normalized_frequency_1000', y='word', data=df.head(n))\n",
    "        plt.title(f'Top {n} Most Frequent Words (Normalized per 1000 words)')\n",
    "        plt.xlabel('Normalized Frequency')\n",
    "    else:\n",
    "        sns.barplot(x='frequency', y='word', data=df.head(n))\n",
    "        plt.title(f'Top {n} Most Frequent Words')\n",
    "        plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()\n",
    "\n",
    "def plot_word_length_distribution(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df['length'], bins=20, kde=False)\n",
    "    plt.title('Distribution of Word Lengths')\n",
    "    plt.xlabel('Word Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "def plot_frequency_vs_length(df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='length', y='frequency', data=df)\n",
    "    plt.title('Word Frequency vs Word Length')\n",
    "    plt.xlabel('Word Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def plot_cumulative_frequency(df):\n",
    "    df_sorted = df.sort_values('frequency', ascending=False)\n",
    "    df_sorted['cumulative_freq'] = df_sorted['frequency'].cumsum() / df_sorted['frequency'].sum() * 100\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, len(df_sorted) + 1), df_sorted['cumulative_freq'])\n",
    "    plt.title('Cumulative Word Frequency')\n",
    "    plt.xlabel('Number of Unique Words')\n",
    "    plt.ylabel('Cumulative Frequency (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "plot_top_words(df,normalized=False)\n",
    "plot_top_words(df,normalized=True)\n",
    "plot_word_length_distribution(df)\n",
    "plot_frequency_vs_length(df)\n",
    "plot_cumulative_frequency(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify potential character names\n",
    "def is_potential_name(word, tag):\n",
    "    return (tag.startswith('NNP') and len(word) > 1 and word[0].isupper()) or word in ['I', 'Me', 'You', 'He', 'She', 'Him', 'Her']\n",
    "\n",
    "# Extract potential character names\n",
    "potential_names = [word for word, tag in tagged_tokens if is_potential_name(word, tag)]\n",
    "\n",
    "# Function to combine first and last names\n",
    "def combine_names(names):\n",
    "    combined_names = []\n",
    "    i = 0\n",
    "    while i < len(names) - 1:\n",
    "        if names[i][0].isupper() and names[i+1][0].isupper():\n",
    "            combined_names.append(f\"{names[i]} {names[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            combined_names.append(names[i])\n",
    "            i += 1\n",
    "    if i == len(names) - 1:\n",
    "        combined_names.append(names[i])\n",
    "    return combined_names\n",
    "\n",
    "# Combine names\n",
    "combined_names = combine_names(potential_names)\n",
    "\n",
    "# Count name occurrences\n",
    "name_counts = Counter(combined_names)\n",
    "\n",
    "# Function to normalize names\n",
    "def normalize_name(name):\n",
    "    # Remove possessive 's\n",
    "    name = re.sub(r\"'s\\b\", \"\", name)\n",
    "    # Split the name into parts\n",
    "    parts = name.split()\n",
    "    # Sort the parts to handle cases like \"Bunyan Paul\"\n",
    "    parts.sort()\n",
    "    # Join the parts back together\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Function to combine similar names\n",
    "def combine_similar_names(name_counts):\n",
    "    normalized_counts = {}\n",
    "    for name, count in name_counts.items():\n",
    "        normalized_name = normalize_name(name)\n",
    "        if normalized_name in normalized_counts:\n",
    "            normalized_counts[normalized_name] += count\n",
    "        else:\n",
    "            normalized_counts[normalized_name] = count\n",
    "    return normalized_counts\n",
    "\n",
    "# Combine Similar Names\n",
    "name_counts = combine_similar_names(name_counts)\n",
    "\n",
    "# Remove nonsensical combinations\n",
    "name_counts = {name: count for name, count in name_counts.items() \n",
    "               if not (name.split()[0] == name.split()[-1] and len(name.split()) > 1)}\n",
    "\n",
    "# Convert to DataFrame\n",
    "name_df = pd.DataFrame.from_dict(name_counts, orient='index', columns=['frequency'])\n",
    "name_df = name_df.sort_values('frequency', ascending=False)\n",
    "name_df['name'] = name_df.index\n",
    "name_df = name_df.reset_index(drop=True)\n",
    "\n",
    "# Calculate percentage of total names\n",
    "total_names = name_df['frequency'].sum()\n",
    "name_df['percentage'] = name_df['frequency'] / total_names * 100\n",
    "\n",
    "\n",
    "\n",
    "# Print top character names\n",
    "print(\"Top 20 Potential Character Names:\")\n",
    "print(name_df[['name', 'frequency', 'percentage']].head(20).to_string(index=False))\n",
    "\n",
    "# Visualization for character names\n",
    "def plot_top_names(df, n=20):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='frequency', y='name', data=df.head(n))\n",
    "    plt.title(f'Top {n} Most Frequent Character Names')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Name')\n",
    "    plt.show()\n",
    "\n",
    "plot_top_names(name_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "def get_character_sentiment(character, text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentences = sent_tokenize(text)\n",
    "    character_sentences = [sent for sent in sentences if character.lower() in sent.lower()]\n",
    "    if not character_sentences:\n",
    "        return {'compound': 0, 'pos': 0, 'neu': 0, 'neg': 0}\n",
    "    sentiments = [sia.polarity_scores(sent) for sent in character_sentences]\n",
    "    avg_sentiment = {key: sum(sent[key] for sent in sentiments) / len(sentiments) \n",
    "                     for key in sentiments[0]}\n",
    "    return avg_sentiment\n",
    "\n",
    "# Get the top 20 characters\n",
    "top_20_characters = name_df.head(20)\n",
    "\n",
    "# Add sentiment analysis to the DataFrame\n",
    "for index, row in top_20_characters.iterrows():\n",
    "    sentiment = get_character_sentiment(row['name'], cleaned_text)\n",
    "    name_df.at[index, 'sentiment_compound'] = sentiment['compound']\n",
    "    name_df.at[index, 'sentiment_positive'] = sentiment['pos']\n",
    "    name_df.at[index, 'sentiment_neutral'] = sentiment['neu']\n",
    "    name_df.at[index, 'sentiment_negative'] = sentiment['neg']\n",
    "\n",
    "# Print top character names with sentiment\n",
    "print(\"Top 20 Potential Character Names with Sentiment:\")\n",
    "print(name_df[['name', 'frequency', 'percentage', 'sentiment_compound']].head(20).to_string(index=False))\n",
    "\n",
    "# Visualization for character names and sentiment\n",
    "def plot_top_names_sentiment(df, n=20):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='frequency', y='sentiment_compound', size='frequency', \n",
    "                    hue='sentiment_compound', data=df.head(n), legend=False)\n",
    "    plt.title(f'Top {n} Most Frequent Character Names with Sentiment')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Sentiment (Compound Score)')\n",
    "    \n",
    "    for i, row in df.head(n).iterrows():\n",
    "        plt.annotate(row['name'], (row['frequency'], row['sentiment_compound']))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_top_names_sentiment(name_df)\n",
    "\n",
    "# Additional visualization: Sentiment distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=name_df[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative']])\n",
    "plt.title('Distribution of Sentiment Scores Across Characters')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
